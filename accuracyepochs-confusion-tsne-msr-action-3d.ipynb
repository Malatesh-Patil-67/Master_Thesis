{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7600993,"sourceType":"datasetVersion","datasetId":4424837}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch_dct\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-24T15:29:13.202343Z","iopub.execute_input":"2024-02-24T15:29:13.203048Z","iopub.status.idle":"2024-02-24T15:29:26.064894Z","shell.execute_reply.started":"2024-02-24T15:29:13.203018Z","shell.execute_reply":"2024-02-24T15:29:26.063765Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torch_dct\n  Downloading torch_dct-0.1.6-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from torch_dct) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch_dct) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch_dct) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch_dct) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch_dct) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch_dct) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->torch_dct) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.1->torch_dct) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.1->torch_dct) (1.3.0)\nDownloading torch_dct-0.1.6-py3-none-any.whl (5.1 kB)\nInstalling collected packages: torch_dct\nSuccessfully installed torch_dct-0.1.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:29:27.860900Z","iopub.execute_input":"2024-02-24T15:29:27.861796Z","iopub.status.idle":"2024-02-24T15:29:40.014726Z","shell.execute_reply.started":"2024-02-24T15:29:27.861760Z","shell.execute_reply":"2024-02-24T15:29:40.013617Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install timm","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:29:43.829634Z","iopub.execute_input":"2024-02-24T15:29:43.830017Z","iopub.status.idle":"2024-02-24T15:29:55.993436Z","shell.execute_reply.started":"2024-02-24T15:29:43.829984Z","shell.execute_reply":"2024-02-24T15:29:55.992299Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.12)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (4.66.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.24.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport logging\nfrom functools import partial\nfrom collections import OrderedDict\nfrom einops import rearrange, repeat\n\nimport torch\nimport torch_dct as dct\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.helpers import load_pretrained\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\n\nimport os\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n\n\n\n\n#from google.colab import drive\n\n# This will prompt for authorization\n#drive.mount('/content/drive')\n\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass FreqMlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        b, f, _ = x.shape\n        x = dct.dct(x.permute(0, 2, 1)).permute(0, 2, 1).contiguous()\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        x = dct.idct(x.permute(0, 2, 1)).permute(0, 2, 1).contiguous()\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass MixedBlock(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp1 = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.norm3 = norm_layer(dim)\n        self.mlp2 = FreqMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        b, f, c = x.shape\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x1 = x[:, :f//2] + self.drop_path(self.mlp1(self.norm2(x[:, :f//2])))\n        x2 = x[:, f//2:] + self.drop_path(self.mlp2(self.norm3(x[:, f//2:])))\n        return torch.cat((x1, x2), dim=1)\n\n\nclass PoseTransformerV2(nn.Module):\n    def __init__(self, num_frame=31, num_joints=20, in_chans=3, embed_dim_ratio=32, depth=4, num_frame_kept=27, num_coeff_kept=27,\n                 num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,  norm_layer=None, num_classes = 0):\n        \"\"\"    ##########hybrid_backbone=None, representation_size=None,\n        Args:\n            num_frame (int, tuple): input frame number\n            num_joints (int, tuple): joints number\n            in_chans (int): number of input channels, 2D joints have 2 channels: (x,y)\n            embed_dim_ratio (int): embedding dimension ratio\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer\n        \"\"\"\n        super().__init__()\n\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        embed_dim = embed_dim_ratio * num_joints   #### temporal embed_dim is num_joints * spatial embedding dim ratio\n        out_dim = num_classes    #### output dimension is num_joints * 3\n\n        self.num_frame_kept = num_frame_kept\n        self.num_coeff_kept = num_coeff_kept\n\n\n\n        ### spatial patch embedding\n        self.Joint_embedding = nn.Linear(in_chans, embed_dim_ratio)\n        self.Freq_embedding = nn.Linear(in_chans*num_joints, embed_dim)\n\n        self.Spatial_pos_embed = nn.Parameter(torch.zeros(1, num_joints, embed_dim_ratio))\n        self.Temporal_pos_embed = nn.Parameter(torch.zeros(1, self.num_frame_kept, embed_dim))\n        self.Temporal_pos_embed_ = nn.Parameter(torch.zeros(1, self.num_coeff_kept, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\n        self.Spatial_blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim_ratio, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n\n        self.blocks = nn.ModuleList([\n            MixedBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n\n        self.Spatial_norm = norm_layer(embed_dim_ratio)\n        self.Temporal_norm = norm_layer(embed_dim)\n\n        ####### A easy way to implement weighted mean\n        self.weighted_mean = torch.nn.Conv1d(in_channels=self.num_coeff_kept, out_channels=1, kernel_size=1)\n        self.weighted_mean_ = torch.nn.Conv1d(in_channels=self.num_frame_kept, out_channels=1, kernel_size=1)\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(embed_dim*2),\n            nn.Linear(embed_dim*2, out_dim),\n        )\n\n    def Spatial_forward_features(self, x):\n        b, f, p, _ = x.shape  ##### b is batch size, f is number of frames, p is number of joints\n        num_frame_kept = self.num_frame_kept\n\n        index = torch.arange((f-1)//2-num_frame_kept//2, (f-1)//2+num_frame_kept//2+1)\n\n        x = self.Joint_embedding(x[:, index].view(b*num_frame_kept, p, -1))\n\n\n        x += self.Spatial_pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.Spatial_blocks:\n            x = blk(x)\n\n        x = self.Spatial_norm(x)\n        x = rearrange(x, '(b f) p c -> b f (p c)', f=num_frame_kept)\n        return x\n\n    def forward_features(self, x, Spatial_feature):\n        b, f, p, _ = x.shape\n        num_coeff_kept = self.num_coeff_kept\n\n        x = dct.dct(x.permute(0, 2, 3, 1))[:, :, :, :num_coeff_kept]\n        x = x.permute(0, 3, 1, 2).contiguous().view(b, num_coeff_kept, -1)\n        x = self.Freq_embedding(x)\n\n        Spatial_feature += self.Temporal_pos_embed\n        x += self.Temporal_pos_embed_\n        x = torch.cat((x, Spatial_feature), dim=1)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.Temporal_norm(x)\n        return x\n\n    def forward(self, x):\n        b, f, p, _ = x.shape\n        x_ = x.clone()\n\n        Spatial_feature = self.Spatial_forward_features(x)\n        x = self.forward_features(x_, Spatial_feature)\n        x = torch.cat((self.weighted_mean(x[:, :self.num_coeff_kept]), self.weighted_mean_(x[:, self.num_coeff_kept:])), dim=-1)\n\n        x = self.head(x)\n\n        x = x.view(b, num_classes)\n\n        return x\n\n\nclass MyCustomDataset(Dataset):\n    def __init__(self, root_dir, mode='train', sliding_window_size=31):\n        self.root_dir = root_dir\n        self.subjects = os.listdir(root_dir)\n        self.frame_data = []  # Stores the information of each frame\n        self.labels = [] # Stores the labels of each frame\n        self.indexvalues = []\n        self.joint_coordinates = []\n        self.sliding_window_size = sliding_window_size\n        self._process_data(mode)\n\n    def __len__(self):\n        return len(self.frame_data)\n    \n\n\n    def __getitem__(self, index):\n        #print(\"index\")\n        #print(index)\n        center_frame = self.frame_data[index]\n        #print(\"center_frame\")\n        #print(center_frame)\n        label = self.labels[index]\n        #print(\"label\")\n        #print(label)\n        skeleton_number = center_frame['skeleton_number']\n\n        #print(\"skeleton_number\")\n        #print(skeleton_number)\n\n\n        skeleton_start_index = self.indexvalues[skeleton_number]['start_index']\n        skeleton_end_index = self.indexvalues[skeleton_number]['end_index']\n\n        #print(\"skeleton_start_index\")\n        #print(skeleton_start_index)\n\n        #print(\"skeleton_end_index\")\n        #print(skeleton_end_index)\n\n        start_index = index - self.sliding_window_size // 2\n        end_index = index + self.sliding_window_size // 2\n\n        #print(\"start_index\")\n        #print(start_index)\n\n        #print(\"end_index\")\n        #print(end_index)\n\n        if (start_index < skeleton_start_index):\n          start_index = skeleton_start_index\n\n        else:\n          start_index = max(start_index, 0)\n\n        if (end_index > skeleton_end_index):\n          end_index = skeleton_end_index\n\n\n\n        #print(\"start_index\")\n        #print(start_index)\n\n        #print(\"end_index\")\n        #print(end_index)\n\n        t = []\n\n        for i in range(start_index, end_index + 1):\n          frame_data = self.frame_data[i]\n          #print(frame_data['joint_coordinates'])\n          joint_coordinates = frame_data['joint_coordinates']\n          t.append(joint_coordinates)\n\n        #print(\"t\")\n        #print(t)\n        #print(len(t))\n        while len(t) < self.sliding_window_size:\n          t.append([0.0] * 60)  # Assuming 25 joints with (x, y, z) coordinates\n\n        if len(t) < 31 and isinstance(t[0], list):  # Check if t contains elements that are lists\n            print(\"Shape of t before tensor conversion:\", len(t), len(t[0]))\n        #else:\n            #print(\"Shape of t before tensor conversion:\", len(t))\n\n        t = torch.tensor(t, dtype=torch.float32)\n        t = t.view(self.sliding_window_size, 20, 3)\n\n        #print(input(\"Hi\"))\n\n        return t, label\n\n    def _process_data(self, mode):\n        frame_counter = 0  # Counter for sequential frame numbering\n        skeleton_temp=0\n        indices = []\n        counting = 0\n\n        if mode == 'train':\n            data_dir = os.path.join(self.root_dir, 'train')\n        elif mode == 'test':\n            data_dir = os.path.join(self.root_dir, 'test')\n        else:\n            raise ValueError(\"Invalid mode. Mode should be 'train' or 'test'.\")\n\n\n\n        skeleton_folder = os.listdir(data_dir)\n\n        for skeleton_file in skeleton_folder:\n          skeleton_dir = os.path.join(data_dir, skeleton_file)\n\n\n          action_folder = skeleton_dir[-23:-21]\n          #print(action_folder)\n          #print(input(\"hi\"))\n              \n                \n        \n\n          # Read and process the skeleton.txt file to extract frame-level data\n          frames, labels , start , end= self._read_skeleton_file(skeleton_dir, frame_counter, action_folder , skeleton_temp)\n          counting = counting +1\n          #print(counting)\n          indices.append({\n          'start_index': start,\n          'end_index': end,\n\n          })\n          self.frame_data.extend(frames)\n          self.labels.extend(labels)\n\n          # Update the frame counter based on the number of frames in the current skeleton file\n          frame_counter += len(frames)\n          skeleton_temp = skeleton_temp + 1\n\n        self.indexvalues.extend(indices)\n\n\n    def _read_skeleton_file(self, skeleton_file, frame_counter, action , skeleton_temp):\n        frames = []\n        labels = []\n        start_ind = frame_counter\n        temp_index = 0\n        \n        #print(skeleton_file)\n        # Read and parse the skeleton.txt file to extract frame-level data\n\n        with open(skeleton_file, 'r') as f:\n          fcounter = 0\n          #first_line = f.readline()\n          #first_value = int(first_line.strip())\n          #print(\"frames\")\n          #print(first_value)\n\n\n          lines = f.readlines()\n\n\n          line_counter = 0\n\n\n          #v = lines[line_counter].strip().split()\n\n\n          while line_counter < (len(lines) - 20):\n\n            v = lines[line_counter].strip().split()\n\n            #if( v[0] == \"0\"):\n            #line_counter = line_counter+1\n            #continue\n\n            if skeleton_file == \"/kaggle/input/mal-zip/nt60/TrainingData/S007C001P017R002A042.skeleton\":\n                print(line_counter)\n                print(lines[line_counter])\n            #line_counter += 3\n            \n\n\n            frame_number = frame_counter + fcounter\n            fcounter=fcounter + 1\n            coordinates = []\n            # Process the next 25 lines\n            for i in range(20):\n              # Make sure we don't go beyond the total number of lines in the fil\n\n              if line_counter < len(lines):\n                if skeleton_file == \"/kaggle/input/mal-zip/nt60/TrainingData/S007C001P017R002A042.skeleton\":\n                    print(line_counter)\n                current_line = lines[line_counter].strip().split()\n\n                # Process the content of the current line here\n                for k in current_line[0:3]:\n                  #print(k)\n                  #print(input(\"wait\"))\n                \n                  \n                  coordinates.append(float(k))\n                  #print(coordinates)\n                  self.joint_coordinates.append(coordinates)\n                  #self.joint_coordinates.append(coordinates)\n                  #if skeleton_file == \"/kaggle/input/mal-zip/nt60/TrainingData/S007C001P017R002A042.skeleton\":\n                  #print(coordinates)\n                  #print(current_line)  # For example, print the content for demonstration\n              line_counter = line_counter + 1\n\n            if skeleton_file == \"/kaggle/input/mal-zip/nt60/TrainingData/S007C001P017R002A042.skeleton\":\n                print(\"all coordinates\")\n                print(coordinates)\n\n            # Append the frame data and label to the respective lists\n            #print(\"frame number\")\n            #print(frame_number)\n            #print(coordinates)\n            #print(input(\"wait\"))\n            frames.append({\n            'frame_number': frame_number,\n            'joint_coordinates': coordinates,\n            'skeleton_number' : skeleton_temp,\n\n            })\n            label = action\n            labels.append(label)\n            # Move to the next block of 3 lines\n            #print(self.joint_coordinates)\n\n\n            if (len(coordinates) != 60):\n                print(len(coordinates))\n                print(\"Not correct\")\n                print(skeleton_file)\n                print(input(\"Hi not correct\"))\n\n            if skeleton_file == \"/kaggle/input/mal-zip/nt60/TrainingData/S007C001P017R002A042.skeleton\":\n                print(input(\"hi end of while\"))\n\n\n\n\n\n\n\n\n        if skeleton_file == \"/kaggle/input/mal-zip/nt60/TrainingData/S007C001P017R002A042.skeleton\":\n            print(input(\"hi end of file\"))\n\n        end_ind = frame_number\n\n\n\n        temp_index = frame_number\n\n        #print(coordinates)\n\n        if skeleton_file == \"/kaggle/input/mal-zip/nt60/TrainingData/S007C001P017R002A042.skeleton\":\n            print(\"start_ind\")\n            print(start_ind)\n            print(\"temp_index\")\n            print(temp_index)\n            print(input(\"wait\"))\n\n\n        #print(input(\"wait\"))\n        return frames, labels, start_ind, temp_index\n\n\n\nprint(\"hey\")\n\n# Set the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n\n\n\n# Set the root directory of your dataset\ntrain_root_dir = '/kaggle/input/msr-action-3d/msr action 3d skeleton split'\n\ntest_root_dir = '/kaggle/input/msr-action-3d/msr action 3d skeleton split'\n\n#train_root_dir = '/content/drive/My Drive/nt60'\n\n#test_root_dir = '/content/drive/My Drive/nt60'\n\n\nprint(\"train\")\n# Create an instance of the training dataset\ntrain_dataset = MyCustomDataset(train_root_dir, mode='train')\nprint(input(\"train dataset done\"))\nprint(\"test\")\n# Create an instance of the testing dataset\ntest_dataset = MyCustomDataset(test_root_dir, mode='test')\nprint(input(\"test dataset done\"))\n# Set the batch size\nbatch_size = 32\n\n# Create data loaders\nprint(\"train dataloader\")\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nprint(input(\"train dataloader done\"))\n\nprint(\"test dataloader\")\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\nprint(input(\"test dataloader done\"))\n\ntrain_dataset_length = len(train_dataset)\n\ntest_dataset_length = len(test_dataset)\n\nprint(\"train_dataset_length\")\nprint(train_dataset_length)\nprint(\"test_dataset_length\")\nprint(test_dataset_length)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:30:27.866740Z","iopub.execute_input":"2024-02-24T15:30:27.867166Z","iopub.status.idle":"2024-02-24T15:30:45.555778Z","shell.execute_reply.started":"2024-02-24T15:30:27.867131Z","shell.execute_reply":"2024-02-24T15:30:45.554780Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"hey\ncuda\ntrain\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"train dataset done m\n"},{"name":"stdout","text":"m\ntest\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"test dataset done m\n"},{"name":"stdout","text":"m\ntrain dataloader\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"train dataloader done m\n"},{"name":"stdout","text":"m\ntest dataloader\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"test dataloader done m\n"},{"name":"stdout","text":"m\ntrain_dataset_length\n13068\ntest_dataset_length\n9162\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nfrom sklearn.manifold import TSNE\n\nimport matplotlib\nmatplotlib.use('Agg')  # Use the Agg backend\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.cm as cm\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\n\nimport seaborn as sns\nfrom scipy.stats import gaussian_kde\nfrom scipy.optimize import brentq\nfrom scipy.stats import gaussian_kde\nfrom scipy.ndimage import gaussian_filter1d\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\n\n\n# Set the number of output classes\nnum_classes = 20 # Number of classes in dataset\nemb_dim = num_classes\n\n# Set the hyperparameters\nnum_frame =31\nnum_joints = 20\nin_chans = 3\nembed_dim_ratio = 64\nnum_frame_kept=27\nnum_coeff_kept=27\ndepth = 4\nnum_heads = 8\nmlp_ratio = 2.\nqkv_bias = True\nqk_scale = None\ndrop_rate = 0.\nattn_drop_rate = 0.\ndrop_path_rate = 0.2\n\n\n\n# Create an instance of the PoseTransformer model\nmodel = PoseTransformerV2(num_frame=num_frame, num_joints=num_joints, in_chans=in_chans, embed_dim_ratio=embed_dim_ratio,\n                          num_frame_kept=num_frame_kept,num_coeff_kept=num_coeff_kept,\n                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate,\n                        num_classes =num_classes)\n\n# Move the model to the device\nmodel = model.to(device)\n\n# Set the optimizer and learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Set the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Set the number of training epochs\nnum_epochs = 5\n\n#print(train_dataset.labels)\n#print(input(\"hi\"))\n\n# Create a label encoder\nlabel_encoder = LabelEncoder()\n\n# Fit label encoder on the training labels\nlabel_encoder.fit(train_dataset.labels)\n\n# Convert training labels to numerical values\ntrain_labels_encoded = label_encoder.transform(train_dataset.labels)\n\n# Convert the encoded labels to tensors\ntrain_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long).to(device)\n\nprint(\"Hi\")\nprint(num_frame)\n\ntrain_accuracies = []\n\n# Training loop\nfor epoch in range(num_epochs):\n    print(epoch)\n    model.train()  # Set the model to training mode\n    train_loss = 0.0\n    train_correct = 0\n    \n    all_embeddings_tsne = []\n\n    for inputs, labels in train_dataloader:\n        inputs = inputs.to(device)\n\n        labels_encoded = label_encoder.transform(labels)  # Convert current batch labels to numerical values\n        labels_tensor = torch.tensor(labels_encoded, dtype=torch.long).to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        \n        embeddings = outputs\n        \n        embeddings_tsne = embeddings.view(-1,1,emb_dim)\n        \n        all_embeddings_tsne.append(embeddings_tsne)\n\n        # Compute the loss\n        loss = criterion(outputs, labels_tensor)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Compute the training accuracy\n        _, predicted = torch.max(outputs, 1)\n        train_correct += (predicted == labels_tensor).sum().item()\n\n        # Update the training loss\n        train_loss += loss.item() * inputs.size(0)\n\n    # Compute the average training loss and accuracy\n    train_loss = train_loss / len(train_dataset)\n    train_accuracy = train_correct / len(train_dataset)\n    train_accuracies.append(train_accuracy)\n\n    # Print the training loss and accuracy for each epoch\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n\n    \nall_embeddings_tsne = torch.cat(all_embeddings_tsne, dim=0)\n\ntorch.save(all_embeddings_tsne , '31_5_final_embeddings.pth')\n\n# Save the trained model\ntorch.save(model.state_dict(), 'trained_msr_action_3d_31_5_pose_transformer_model.pth')\n\n\n\n# Testing loop\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\ntest_correct = 0\ntest_predictions = []\ntest_targets = []\ntest_lab_pred =[]\ntrue_lab=[]\n\nall_test_embeddings = []\n\nwith torch.no_grad():\n    for inputs, labels in test_dataloader:\n        inputs = inputs.to(device)\n\n        labels_encoded = label_encoder.transform(labels)  # Convert current batch labels to numerical values\n        labels_tensor = torch.tensor(labels_encoded, dtype=torch.long).to(device)\n\n\n        # Forward pass\n        outputs = model(inputs)\n        \n        embeddings = outputs\n        \n        embeddings_tsne = embeddings.view(-1,1,emb_dim)\n        \n        all_test_embeddings.append(embeddings_tsne)\n\n        # Compute the loss\n        loss = criterion(outputs, labels_tensor)\n\n        # Compute the testing accuracy\n        _, predicted = torch.max(outputs, 1)\n        test_correct += (predicted == labels_tensor).sum().item()\n\n        # Update the testing loss\n        test_loss += loss.item() * inputs.size(0)\n\n        # Collect predictions and targets for computing metrics\n        test_predictions.extend(predicted.cpu().numpy())\n        test_targets.extend(labels_encoded)\n        \n        test_lab_pred.append(labels_tensor)\n        \n        true_lab.append(labels)\n\n    # Compute the average testing loss and accuracy\n    test_loss = test_loss / len(test_dataset)\n    test_accuracy = test_correct / len(test_dataset)\n\n    # Print the testing loss and accuracy for each epoch\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T16:40:38.338215Z","iopub.execute_input":"2024-02-24T16:40:38.338596Z","iopub.status.idle":"2024-02-24T16:46:04.207136Z","shell.execute_reply.started":"2024-02-24T16:40:38.338567Z","shell.execute_reply":"2024-02-24T16:46:04.205968Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Hi\n31\n0\nEpoch [1/5], Train Loss: 3.0593, Train Accuracy: 0.1472\n1\nEpoch [2/5], Train Loss: 2.0872, Train Accuracy: 0.3776\n2\nEpoch [3/5], Train Loss: 1.2036, Train Accuracy: 0.6086\n3\nEpoch [4/5], Train Loss: 0.9825, Train Accuracy: 0.6735\n4\nEpoch [5/5], Train Loss: 0.7915, Train Accuracy: 0.7281\nTest Loss: 1.3036, Test Accuracy: 0.6359\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_accuracy_vs_epochs(train_accuracies, num_epochs):\n    plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n    plt.plot(range(1, num_epochs + 1), train_accuracies, color='orange', marker='o', linestyle='-')  # Adjust line color as needed\n    plt.xlabel('Epochs', fontsize=12)  # Adjust font size as needed\n    plt.ylabel('Accuracy', fontsize=12)  # Adjust font size as needed\n    plt.title('Training Accuracy vs Epochs', fontsize=14, fontweight='bold')  # Adjust font size and weight as needed\n    plt.xticks(fontsize=10)  # Adjust font size of x-axis ticks as needed\n    plt.yticks(fontsize=10)  # Adjust font size of y-axis ticks as needed\n    #plt.grid(True, linestyle='--', alpha=0.7)  # Add grid lines with custom style and transparency\n    plt.gca().set_facecolor('black')  # Set background color to light yellow\n    plt.tight_layout()  # Adjust layout to prevent overlap of labels\n    plt.savefig('train_accuracy_vs_epochs.png', dpi=300)  # Save plot with high resolution\n    plt.close()  # Close the figure to release memory resources\n\n# Example usage:\nplot_accuracy_vs_epochs(train_accuracies, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T17:12:50.683164Z","iopub.execute_input":"2024-02-24T17:12:50.683884Z","iopub.status.idle":"2024-02-24T17:12:51.194884Z","shell.execute_reply.started":"2024-02-24T17:12:50.683851Z","shell.execute_reply":"2024-02-24T17:12:51.194028Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nall_embeddings_tsne = torch.cat(all_test_embeddings, dim=0)\n\nembeds = all_embeddings_tsne\n\ntargets = torch.cat(test_lab_pred, dim=0)\n\n\ncustom_colors = ['#a26989', '#924ff1', '#6029e8', '#f0215b', '#8da6ae', '#fc8cd9', '#5cc8ff', '#d2cd29', '#fc85a8', '#a55e73', '#89b38f', '#3c2b60', '#27ba26', '#5b274d', '#84421a', '#c142c0', '#97bf08', '#fe978e', '#c3dd15', '#5b7ff6']\n\n# Convert anchor_embeddings to numpy array\nembeddings_np = embeds.cpu().numpy()  # Assuming anchor_embeddings is a PyTorch tensor\n\n#Flatten the anchor embeddings to a 2D array\nflattened_embeddings = embeds.view(embeds.size(0), -1).cpu().numpy()\n\n\n# Get the labels for the tsne embeddings\ntsne_labels = targets.cpu().numpy()\n\n\nperplexity_values = [20, 30, 50]\niteration_values = [500, 1000]\n\n\nplt.figure(figsize=(10, 10))\nplot_num = 1\nnum= 1\n\ncmap = ListedColormap(custom_colors[:len(np.unique(tsne_labels))])\n\nfor perplexity in perplexity_values:\n    for iterations in iteration_values:\n        tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=iterations, random_state=42)\n        tsne_embeddings = tsne.fit_transform(flattened_embeddings)\n        plt.subplot(len(perplexity_values), len(iteration_values), plot_num)\n        plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=tsne_labels, cmap=cmap , s=1.0)\n        plt.title(f'Perplexity: {perplexity}, Iterations: {iterations}')\n        plt.show()\n        plt.xticks([])\n        plt.yticks([])\n        plt.savefig('2_tsne_plot' + str(num) + '.png', dpi=300)\n        plot_num += 1\n        num = num+1\n        \nplt.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T17:04:13.357056Z","iopub.execute_input":"2024-02-24T17:04:13.357422Z","iopub.status.idle":"2024-02-24T17:07:35.808880Z","shell.execute_reply.started":"2024-02-24T17:04:13.357394Z","shell.execute_reply":"2024-02-24T17:07:35.807787Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Define custom colors and action names\ncustom_colors = ['#a26989', '#924ff1', '#6029e8', '#f0215b', '#8da6ae', \n                 '#fc8cd9', '#5cc8ff', '#d2cd29', '#fc85a8', '#a55e73', \n                 '#89b38f', '#3c2b60', '#27ba26', '#5b274d', '#84421a', \n                 '#c142c0', '#97bf08', '#fe978e', '#c3dd15', '#5b7ff6']\naction_names = [\n    \"high arm wave\", \"horizontal arm wave\", \"hammer\", \"hand catch\", \"forward punch\",\n    \"high throw\", \"draw x\", \"draw tick\", \"draw circle\", \"hand clap\", \"two hand wave\",\n    \"side-boxing\", \"bend\", \"forward kick\", \"side kick\", \"jogging\", \"tennis swing\",\n    \"tennis serve\", \"golf swing\", \"pick up & throw\"\n]\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot each color with its corresponding action name using circles\nfor i, (color, action) in enumerate(zip(custom_colors, action_names)):\n    ax.scatter([i]*2, [0, 1], color=color, label=action, marker='o', s=200)  # Use circles as markers\n\n# Hide the axes\nax.axis('off')\n\n# Add legend with adjusted font size\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='large')  # You can adjust font size here\n\n# Save the plot as an image\nplt.savefig('color_action_plot.png', bbox_inches='tight')\n\n# Show plot\nplt.tight_layout()\nplt.show()\n\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T16:54:07.878318Z","iopub.execute_input":"2024-02-24T16:54:07.879342Z","iopub.status.idle":"2024-02-24T16:54:08.645506Z","shell.execute_reply.started":"2024-02-24T16:54:07.879306Z","shell.execute_reply":"2024-02-24T16:54:08.644652Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\n\naction_names = [\n    \"high arm wave\",\n    \"horizontal arm wave\",\n    \"hammer\",\n    \"hand catch\",\n    \"forward punch\",\n    \"high throw\",\n    \"draw x\",\n    \"draw tick\",\n    \"draw circle\",\n    \"hand clap\",\n    \"two hand wave\",\n    \"side-boxing\",\n    \"bend\",\n    \"forward kick\",\n    \"side kick\",\n    \"jogging\",\n    \"tennis swing\",\n    \"tennis serve\",\n    \"golf swing\",\n    \"pick up & throw\"\n]\n\n# Assuming you have the label encoder object available\n#class_names = label_encoder.classes_\n\n# Print or use the class names as needed\n#print(test_lab_pred)\n#print(class_names)\n#print(input(\"hi\"))\n#print(true_lab)\n#print(input(\"hi\"))\n# Get the predictions and true labels\n\npredictions = np.array(test_predictions)\ntrue_labels = np.array(test_targets)\n\n\n\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predictions)\n\n# Define custom colormap from black to orange\ncolors = [\"black\", \"orange\"]\ncmap = LinearSegmentedColormap.from_list(\"Custom\", colors)\n\n# Compute confusion matrix as percentages\nconf_matrix_percentage = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n\n# Plot confusion matrix with actual action names\nplt.figure(figsize=(10, 8))\nheatmap = sns.heatmap(conf_matrix_percentage, annot=True, fmt='.1f', cmap=cmap, xticklabels=action_names, yticklabels=action_names)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.xticks(rotation=90, fontsize=7)  # Adjust font size here\nplt.yticks(rotation=0, fontsize=7)  # Adjust font size here\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('confusion_matrix_percentage.png', dpi=300)\n\n# Show the plot\nplt.show()\n\nplt.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T17:02:57.485530Z","iopub.execute_input":"2024-02-24T17:02:57.485921Z","iopub.status.idle":"2024-02-24T17:02:59.302466Z","shell.execute_reply.started":"2024-02-24T17:02:57.485893Z","shell.execute_reply":"2024-02-24T17:02:59.301390Z"},"trusted":true},"execution_count":42,"outputs":[]}]}